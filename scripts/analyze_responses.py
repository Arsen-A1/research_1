#!/usr/bin/env python3
"""
Response Analysis Pipeline
===========================
Analyzes LLM responses for cross-lingual bias indicators:
- Response length (characters, words, tokens)
- Disclaimer / hedging detection
- Assertiveness / confidence scoring
- Sentiment polarity estimation
- Category-level aggregation

Usage:
    python scripts/analyze_responses.py
    python scripts/analyze_responses.py --models llama3-8b qwen2.5-7b
"""

import json
import re
import argparse
import csv
from pathlib import Path
from collections import defaultdict

import pandas as pd
import numpy as np

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------

ROOT_DIR = Path(__file__).resolve().parent.parent
RESPONSES_DIR = ROOT_DIR / "results" / "responses"
OUTPUT_CSV = ROOT_DIR / "results" / "analysis_summary.csv"

LANGUAGES = ["en", "ru", "zh", "kz"]
LANG_NAMES = {"en": "English", "ru": "Russian", "zh": "Chinese", "kz": "Kazakh"}

# ---------------------------------------------------------------------------
# Disclaimer / Hedging Patterns (multilingual)
# ---------------------------------------------------------------------------

DISCLAIMER_PATTERNS = {
    "en": [
        r"\bi('m| am) (just )?an? (ai|language model|assistant)\b",
        r"\bi (cannot|can't|don't) (provide|give|offer)\b",
        r"\bit('s| is) (important|worth) (to note|noting|mentioning)\b",
        r"\bthis is (a )?(complex|nuanced|subjective)\b",
        r"\bthere (are|is) no (single|one|definitive) (right )?answer\b",
        r"\bI (should|must) (note|mention|clarify)\b",
        r"\bdisclaimer\b",
        r"\bin my opinion\b",
        r"\bsome people (believe|think|argue)\b",
        r"\bit depends on\b",
        r"\bhowever,? (it('s| is)|there)\b",
        r"\bon the other hand\b",
    ],
    "ru": [
        r"\b—è (—è–≤–ª—è—é—Å—å|‚Äî) (–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º|—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é|–∏–∏)\b",
        r"\b–≤–∞–∂–Ω–æ (–æ—Ç–º–µ—Ç–∏—Ç—å|–∑–∞–º–µ—Ç–∏—Ç—å|—É—á–∏—Ç—ã–≤–∞—Ç—å)\b",
        r"\b—ç—Ç–æ (—Å–ª–æ–∂–Ω—ã–π|—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π|–Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π) –≤–æ–ø—Ä–æ—Å\b",
        r"\b–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç\b",
        r"\b–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ (—Å—á–∏—Ç–∞—é—Ç|–ø–æ–ª–∞–≥–∞—é—Ç|–¥—É–º–∞—é—Ç)\b",
        r"\b—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã\b",
        r"\b—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã\b",
        r"\b–∑–∞–≤–∏—Å–∏—Ç –æ—Ç\b",
        r"\b—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å\b",
        r"\b–æ–¥–Ω–∞–∫–æ\b",
    ],
    "zh": [
        r"ÊàëÊòØ(‰∏Ä‰∏™)?‰∫∫Â∑•Êô∫ËÉΩ",
        r"ÊàëÊòØ(‰∏Ä‰∏™)?ËØ≠Ë®ÄÊ®°Âûã",
        r"ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØ",
        r"ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØ",
        r"ËøôÊòØ‰∏Ä‰∏™(Â§çÊùÇ|‰∏ªËßÇ)ÁöÑÈóÆÈ¢ò",
        r"Ê≤°Êúâ(ÂîØ‰∏Ä|Ê†áÂáÜ)ÁöÑÁ≠îÊ°à",
        r"Êúâ‰∫õ‰∫∫ËÆ§‰∏∫",
        r"‰∏ÄÊñπÈù¢.*Âè¶‰∏ÄÊñπÈù¢",
        r"ÂèñÂÜ≥‰∫é",
        r"ÁÑ∂ËÄå",
        r"‰∏çËøá",
    ],
    "kz": [
        r"–º–µ–Ω –∂–∞—Å–∞–Ω–¥—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
        r"–º–∞“£—ã–∑–¥—ã (–µ—Å–∫–µ—Ä—É|–∞—Ç–∞–ø ”©—Ç—É)",
        r"–±“±–ª (–∫“Ø—Ä–¥–µ–ª—ñ|—Å—É–±—ä–µ–∫—Ç–∏–≤—Ç—ñ) –º”ô—Å–µ–ª–µ",
        r"–±—ñ—Ä –∂–∞“õ—Ç—ã –∂–∞—É–∞–ø –∂–æ“õ",
        r"–∫–µ–π–±—ñ—Ä–µ—É–ª–µ—Ä (—Å–∞–Ω–∞–π–¥—ã|–æ–π–ª–∞–π–¥—ã)",
        r"–±—ñ—Ä –∂–∞“ì—ã–Ω–∞–Ω.*–µ–∫—ñ–Ω—à—ñ –∂–∞“ì—ã–Ω–∞–Ω",
        r"–±–∞–π–ª–∞–Ω—ã—Å—Ç—ã",
        r"–∞–ª–∞–π–¥–∞",
        r"–¥–µ–≥–µ–Ω–º–µ–Ω",
    ],
}

# Assertiveness markers ‚Äî confident vs. hedging language
ASSERTIVE_MARKERS = {
    "en": [r"\bdefinitely\b", r"\bclearly\b", r"\bcertainly\b", r"\babsolutely\b",
           r"\bundoubtedly\b", r"\byes\b", r"\bno\b", r"\bthe answer is\b"],
    "ru": [r"\b–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ\b", r"\b–æ—á–µ–≤–∏–¥–Ω–æ\b", r"\b–±–µ–∑—É—Å–ª–æ–≤–Ω–æ\b", r"\b–∫–æ–Ω–µ—á–Ω–æ\b",
           r"\b–¥–∞\b", r"\b–Ω–µ—Ç\b", r"\b–æ—Ç–≤–µ—Ç\b"],
    "zh": [r"ÂΩìÁÑ∂", r"ËÇØÂÆö", r"ÊØ´Êó†ÁñëÈóÆ", r"ÊòæÁÑ∂", r"Á°ÆÂÆû", r"ÊòØÁöÑ", r"‰∏çÊòØ"],
    "kz": [r"”ô—Ä–∏–Ω–µ", r"—Å”©–∑—Å—ñ–∑", r"–∞–Ω—ã“õ", r"–∏”ô", r"–∂–æ“õ", r"–º—ñ–Ω–¥–µ—Ç—Ç—ñ —Ç“Ø—Ä–¥–µ"],
}


def count_words(text: str, language: str) -> int:
    """Count words with language-aware logic."""
    if not text:
        return 0
    if language == "zh":
        # Chinese: approximate by character count (excl. punctuation/spaces)
        return len(re.findall(r"[\u4e00-\u9fff]", text))
    return len(text.split())


def count_disclaimers(text: str, language: str) -> int:
    """Count disclaimer/hedging patterns in text."""
    if not text:
        return 0
    count = 0
    patterns = DISCLAIMER_PATTERNS.get(language, DISCLAIMER_PATTERNS["en"])
    for pattern in patterns:
        count += len(re.findall(pattern, text.lower()))
    return count


def count_assertiveness(text: str, language: str) -> int:
    """Count assertive/confident language markers."""
    if not text:
        return 0
    count = 0
    patterns = ASSERTIVE_MARKERS.get(language, ASSERTIVE_MARKERS["en"])
    for pattern in patterns:
        count += len(re.findall(pattern, text.lower()))
    return count


def compute_confidence_score(text: str, language: str) -> float:
    """
    Compute a confidence score (0-1) based on:
    - High assertiveness ‚Üí higher score
    - Many disclaimers ‚Üí lower score
    """
    if not text:
        return 0.0
    assertive = count_assertiveness(text, language)
    disclaimers = count_disclaimers(text, language)
    total = assertive + disclaimers
    if total == 0:
        return 0.5  # neutral
    return round(assertive / total, 3)


def analyze_response(entry: dict) -> dict:
    """Analyze a single response entry."""
    text = entry.get("answer", "") or ""
    lang = entry.get("language", "en")

    return {
        "question_id": entry["question_id"],
        "category": entry["category"],
        "language": lang,
        "language_name": LANG_NAMES.get(lang, lang),
        "question": entry.get("question", ""),
        "answer_length_chars": len(text),
        "answer_length_words": count_words(text, lang),
        "num_disclaimers": count_disclaimers(text, lang),
        "num_assertive_markers": count_assertiveness(text, lang),
        "confidence_score": compute_confidence_score(text, lang),
        "has_error": entry.get("error") is not None,
        "latency_seconds": entry.get("latency_seconds", 0),
    }


def load_responses(model_key: str) -> dict:
    """Load response file for a model."""
    path = RESPONSES_DIR / f"{model_key}_responses.json"
    if not path.exists():
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def discover_models() -> list[str]:
    """Find all available response files."""
    if not RESPONSES_DIR.exists():
        return []
    models = []
    for path in RESPONSES_DIR.glob("*_responses.json"):
        model_key = path.stem.replace("_responses", "")
        models.append(model_key)
    return sorted(models)


def run_analysis(model_keys: list[str] | None = None):
    """Run full analysis pipeline."""
    if not model_keys:
        model_keys = discover_models()

    if not model_keys:
        print("‚ùå No response files found. Run query_llms.py first.")
        return

    print(f"\n{'='*60}")
    print(f"  Response Analysis Pipeline")
    print(f"{'='*60}")
    print(f"  Models: {', '.join(model_keys)}")
    print(f"{'='*60}\n")

    all_results = []

    for model_key in model_keys:
        data = load_responses(model_key)
        if not data:
            print(f"  ‚ö†Ô∏è  No responses found for: {model_key}")
            continue

        model_name = data.get("model", model_key)
        responses = data.get("responses", [])
        print(f"  üìä Analyzing: {model_name} ({len(responses)} responses)")

        for entry in responses:
            if entry.get("error"):
                continue
            result = analyze_response(entry)
            result["model"] = model_key
            result["model_name"] = model_name
            all_results.append(result)

    if not all_results:
        print("\n‚ùå No valid responses to analyze.")
        return

    # Create DataFrame
    df = pd.DataFrame(all_results)

    # Save full results
    OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(OUTPUT_CSV, index=False)
    print(f"\n  üìÅ Full results saved to: {OUTPUT_CSV}")

    # Print summary statistics
    print(f"\n{'='*60}")
    print(f"  Summary Statistics")
    print(f"{'='*60}\n")

    for model_key in df["model"].unique():
        model_df = df[df["model"] == model_key]
        model_name = model_df["model_name"].iloc[0]
        print(f"\n  ü§ñ {model_name}")
        print(f"  {'‚îÄ'*50}")

        # Per-language stats
        lang_stats = model_df.groupby("language").agg({
            "answer_length_chars": "mean",
            "answer_length_words": "mean",
            "num_disclaimers": "mean",
            "confidence_score": "mean",
        }).round(2)

        print(f"\n  {'Language':<12} {'Avg Chars':>10} {'Avg Words':>10} "
              f"{'Disclaimers':>12} {'Confidence':>12}")
        print(f"  {'‚îÄ'*56}")

        for lang in LANGUAGES:
            if lang in lang_stats.index:
                row = lang_stats.loc[lang]
                print(f"  {LANG_NAMES[lang]:<12} {row['answer_length_chars']:>10.0f} "
                      f"{row['answer_length_words']:>10.0f} "
                      f"{row['num_disclaimers']:>12.2f} "
                      f"{row['confidence_score']:>12.3f}")

        # Per-category stats
        print(f"\n  By category:")
        cat_stats = model_df.groupby("category").agg({
            "answer_length_words": "mean",
            "num_disclaimers": "mean",
            "confidence_score": "mean",
        }).round(2)

        for cat in ["factual", "opinion", "commonsense"]:
            if cat in cat_stats.index:
                row = cat_stats.loc[cat]
                print(f"    {cat:<15} avg_words={row['answer_length_words']:.0f}  "
                      f"disclaimers={row['num_disclaimers']:.2f}  "
                      f"confidence={row['confidence_score']:.3f}")

    # Cross-lingual divergence report
    print(f"\n{'='*60}")
    print(f"  Cross-Lingual Divergence Analysis")
    print(f"{'='*60}\n")

    for model_key in df["model"].unique():
        model_df = df[df["model"] == model_key]
        model_name = model_df["model_name"].iloc[0]
        print(f"  ü§ñ {model_name}")

        # Find questions where response length varies most across languages
        divergence = []
        for qid in model_df["question_id"].unique():
            q_df = model_df[model_df["question_id"] == qid]
            if len(q_df) < 2:
                continue
            length_std = q_df["answer_length_words"].std()
            conf_std = q_df["confidence_score"].std()
            disc_std = q_df["num_disclaimers"].std()
            divergence.append({
                "question_id": qid,
                "category": q_df["category"].iloc[0],
                "length_divergence": round(length_std, 2),
                "confidence_divergence": round(conf_std, 3),
                "disclaimer_divergence": round(disc_std, 2),
                "combined_score": round(length_std * 0.3 + conf_std * 100 * 0.3
                                       + disc_std * 10 * 0.4, 2),
            })

        if divergence:
            div_df = pd.DataFrame(divergence).sort_values(
                "combined_score", ascending=False
            )
            print(f"\n  Top 10 most divergent questions:")
            for i, row in div_df.head(10).iterrows():
                print(f"    Q{row['question_id']:>2d} [{row['category']:<11s}] "
                      f"combined_score={row['combined_score']:.2f}")

    print(f"\n‚úÖ Analysis complete!\n")


def main():
    parser = argparse.ArgumentParser(description="Analyze LLM responses")
    parser.add_argument(
        "--models", nargs="+", default=None,
        help="Model keys to analyze (default: all available)",
    )
    args = parser.parse_args()
    run_analysis(args.models)


if __name__ == "__main__":
    main()
